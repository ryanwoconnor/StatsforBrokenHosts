[Number of Data Points]
action.email.useNSSubject = 1
alert.track = 0
cron_schedule = 0 0 * * *
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
request.ui_dispatch_app = StatsforMissingData
request.ui_dispatch_view = search
search = | tstats count WHERE sourcetype=* earliest=-60d@d by _time host sourcetype index span=30m \
| makecontinuous _time span=30m \
| eval combo=host."&&&&".sourcetype."&&&&".index \
| xyseries _time combo count \
| makecontinuous _time span=30m \
| fillnull value=0 \
| untable _time combo count \
| eval HourOfDay=strftime(_time, "%H") \
| eval BucketMinuteOfHour=strftime(_time, "%M") \
| eval DayOfWeek=strftime(_time, "%A") \
| eval Year=strftime(_time, " %Y") \
| eval DayOfYear=strftime(_time, "%j") \
| eval DayOfWeekNumeric=strftime(_time, "%w") \
| stats avg(count) as avg by HourOfDay BucketMinuteOfHour DayOfWeek DayOfWeekNumeric Year DayOfYear combo \
| sort 0 + combo DayOfYear DayOfWeekNumeric HourOfDay BucketMinuteOfHour \
| fields lowerBound,upperBound,HourOfDay,BucketMinuteOfHour,DayOfWeek,combo, spike, DayOfWeekNumeric, avg avg_stream, avg_overall number_of_data_points, DayOfYear, Year | eval datapoint=case(avg>0, "1", 1=1, "0") | stats sum(datapoint) as sum by combo DayOfWeek HourOfDay BucketMinuteOfHour  | outputlookup number_datapoints_lookup

[Build State for ML]
action.email.useNSSubject = 1
alert.track = 0
cron_schedule = 0 3 * * *
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
request.ui_dispatch_app = StatsforMissingData
request.ui_dispatch_view = search
search = | tstats count WHERE sourcetype=* earliest=-60d@d by _time host sourcetype index span=30m \
| makecontinuous _time span=30m \
| eval combo=host."&&&&".sourcetype."&&&&".index \
| xyseries _time combo count \
| makecontinuous _time span=30m \
| fillnull value=0 \
| untable _time combo count \
| eval HourOfDay=strftime(_time, "%H") \
| eval BucketMinuteOfHour=strftime(_time, "%M") \
| eval DayOfWeek=strftime(_time, "%A") \
| eval DayOfWeekNumeric=strftime(_time, "%w") \
| stats avg(count) as avg stdev(count) as stdev by HourOfDay BucketMinuteOfHour DayOfWeek DayOfWeekNumeric combo \
| sort 0 + combo DayOfWeekNumeric HourOfDay BucketMinuteOfHour \
| streamstats reset_on_change=true window=5 avg(avg) as avg_stream by combo \
| eventstats avg(avg) as avg_overall by combo \
| eval spike=if(avg > 2 * avg_stream, 10000, 0) \
| eval avg=case(spike==10000, avg_overall, 1=1, avg) \
| eval lowerBound=(avg-stdev*exact(2)), upperBound=(avg+stdev*exact(2)) \
| fields lowerBound,upperBound,HourOfDay,BucketMinuteOfHour,DayOfWeek,combo, spike, DayOfWeekNumeric, avg avg_stream, avg_overall, stdev\
| outputlookup state_lookup




[Host Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-15m@m latest=now  by host _indextime \
| bin _indextime span=15m \
| stats count by _indextime, host \
| stats count(host) as M by host \
| addinfo \
| eval N=round((info_max_time-info_min_time)/15/60,0)\
| eval frequency=M/N \
| fields host, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)

[Index Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-15m@m by index _indextime \
| bin _indextime span=15m \
| stats count by _indextime, index \
| stats count(index) as M by index \
| addinfo \
| eval N=round((info_max_time-info_min_time)/15/60,0)\
| eval frequency=M/N \
| fields index, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)

[Sourcetype Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-15m@m by sourcetype _indextime \
| bin _indextime span=15m \
| stats count by _indextime, sourcetype \
| stats count(sourcetype) as M by sourcetype \
| addinfo \
| eval N=round((info_max_time-info_min_time)/15/60,0)\
| eval frequency=M/N \
| fields sourcetype, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)
